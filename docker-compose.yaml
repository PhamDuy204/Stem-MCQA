version: '2.1'

services:
  vllm:
    build: 
      context: .
      dockerfile: vllm.dockerfile
    environment:
      - HF_Token=your_token
    volumes:
      - ./model_cache:/model_cache
    ports: [8000:8000]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  # backend:
    